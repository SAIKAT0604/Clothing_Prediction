{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Clothing_Classification_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAqvSV-zFm19",
        "colab_type": "text"
      },
      "source": [
        "### Clothing_Apparel_Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbjOWtxJFm2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tlPlQtdG1Ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8be94cf8-3060-48f3-f227-4d2917d1afd9"
      },
      "source": [
        "# Load fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbs2QA6qFm26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1c8606d0-1271-4a08-eafb-718bd4c10bdd"
      },
      "source": [
        "# Explore the dataset\n",
        "# Check the shape and size of x_train, x_test, y_train, y_test\n",
        "print (\"Number of samples/observations in training data: \" + str(len(x_train)))\n",
        "print (\"Number of labels in training data: \" + str(len(y_train)))\n",
        "print (\"Dimensions of a single image in x_train:\" + str(x_train[0].shape))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print (\"Number of samples/observations in test data: \" + str(len(x_test)))\n",
        "print (\"Number of labels in test data: \" + str(len(y_test)))\n",
        "print (\"Dimensions of single image in x_test:\" + str(x_test[0].shape))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples/observations in training data: 60000\n",
            "Number of labels in training data: 60000\n",
            "Dimensions of a single image in x_train:(28, 28)\n",
            "-------------------------------------------------------------\n",
            "Number of samples/observations in test data: 10000\n",
            "Number of labels in test data: 10000\n",
            "Dimensions of single image in x_test:(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdXkB2axFm3N",
        "colab_type": "text"
      },
      "source": [
        "### View sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7IrFmtbFm3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "b0c7d20d-f067-4fe4-c68c-35f38e95d2f6"
      },
      "source": [
        "# Visualization library to visualize images \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting 5 images, Subplot arugments represent nrows, ncols and index\n",
        "# Color map is set to grey since our image dataset is grayscale\n",
        "plt.subplot(231)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(232)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(233)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(234)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(235)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "\n",
        "# Visualize the images\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kOiUxZ1Fm3f",
        "colab_type": "text"
      },
      "source": [
        "### Let's create our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdCR3JawFm3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "a86e10dd-5f91-49cd-cf86-29f31014b90b"
      },
      "source": [
        "#Import necessary keras specific libraries\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "# Setting Training Parameters like batch_size, epochs\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Storing the number of rows and columns\n",
        "img_rows = x_train[0].shape[0]\n",
        "img_cols = x_train[1].shape[0]\n",
        "\n",
        "''' Getting the data in the right 'shape' as required by Keras i.e. adding a 4th \n",
        "dimension to our data thereby changing the original image shape of (60000,28,28) \n",
        "to (60000,28,28,1)'''\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "# Storing the shape of a single image \n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Changing image type to float32 data type\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalizing the data by changing the image pixel range from (0 to 255) to (0 to 1)\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Performing one hot encoding\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Calculate the number of classes and number of pixels \n",
        "num_classes = y_test.shape[1]\n",
        "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = keras.optimizers.Adadelta(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,200,778\n",
            "Trainable params: 1,200,330\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzoS-5OcFm3w",
        "colab_type": "text"
      },
      "source": [
        "### Let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bACuyHMEFm3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62bf89b9-6306-4ea0-d69b-cb1c01e8059e"
      },
      "source": [
        "model_fitting = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 1.7925 - accuracy: 0.4411 - val_loss: 0.9907 - val_accuracy: 0.6666\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 1.3286 - accuracy: 0.5727 - val_loss: 0.7995 - val_accuracy: 0.7237\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 1.1342 - accuracy: 0.6325 - val_loss: 0.7151 - val_accuracy: 0.7489\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 1.0160 - accuracy: 0.6670 - val_loss: 0.6631 - val_accuracy: 0.7635\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.9310 - accuracy: 0.6910 - val_loss: 0.6272 - val_accuracy: 0.7765\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.8879 - accuracy: 0.7057 - val_loss: 0.6008 - val_accuracy: 0.7867\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.8389 - accuracy: 0.7201 - val_loss: 0.5787 - val_accuracy: 0.7939\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.8084 - accuracy: 0.7311 - val_loss: 0.5614 - val_accuracy: 0.8013\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.7776 - accuracy: 0.7378 - val_loss: 0.5469 - val_accuracy: 0.8063\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.7523 - accuracy: 0.7454 - val_loss: 0.5344 - val_accuracy: 0.8108\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.7321 - accuracy: 0.7535 - val_loss: 0.5233 - val_accuracy: 0.8146\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.7173 - accuracy: 0.7571 - val_loss: 0.5135 - val_accuracy: 0.8195\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6952 - accuracy: 0.7650 - val_loss: 0.5051 - val_accuracy: 0.8239\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6763 - accuracy: 0.7715 - val_loss: 0.4971 - val_accuracy: 0.8257\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6709 - accuracy: 0.7726 - val_loss: 0.4905 - val_accuracy: 0.8273\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6574 - accuracy: 0.7761 - val_loss: 0.4840 - val_accuracy: 0.8303\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6473 - accuracy: 0.7811 - val_loss: 0.4779 - val_accuracy: 0.8337\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6392 - accuracy: 0.7855 - val_loss: 0.4727 - val_accuracy: 0.8356\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6281 - accuracy: 0.7856 - val_loss: 0.4674 - val_accuracy: 0.8378\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6142 - accuracy: 0.7923 - val_loss: 0.4625 - val_accuracy: 0.8385\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6148 - accuracy: 0.7913 - val_loss: 0.4576 - val_accuracy: 0.8401\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.6013 - accuracy: 0.7950 - val_loss: 0.4535 - val_accuracy: 0.8417\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5917 - accuracy: 0.7982 - val_loss: 0.4496 - val_accuracy: 0.8436\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5870 - accuracy: 0.8010 - val_loss: 0.4457 - val_accuracy: 0.8441\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5797 - accuracy: 0.8039 - val_loss: 0.4421 - val_accuracy: 0.8461\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5766 - accuracy: 0.8056 - val_loss: 0.4387 - val_accuracy: 0.8485\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5711 - accuracy: 0.8069 - val_loss: 0.4352 - val_accuracy: 0.8495\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5608 - accuracy: 0.8094 - val_loss: 0.4322 - val_accuracy: 0.8517\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5595 - accuracy: 0.8116 - val_loss: 0.4295 - val_accuracy: 0.8515\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5570 - accuracy: 0.8122 - val_loss: 0.4262 - val_accuracy: 0.8536\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5471 - accuracy: 0.8142 - val_loss: 0.4240 - val_accuracy: 0.8538\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5431 - accuracy: 0.8152 - val_loss: 0.4208 - val_accuracy: 0.8549\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.5378 - accuracy: 0.8162 - val_loss: 0.4182 - val_accuracy: 0.8561\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5375 - accuracy: 0.8180 - val_loss: 0.4161 - val_accuracy: 0.8566\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5299 - accuracy: 0.8194 - val_loss: 0.4139 - val_accuracy: 0.8568\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5276 - accuracy: 0.8203 - val_loss: 0.4112 - val_accuracy: 0.8593\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5240 - accuracy: 0.8230 - val_loss: 0.4095 - val_accuracy: 0.8583\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5189 - accuracy: 0.8245 - val_loss: 0.4074 - val_accuracy: 0.8599\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5165 - accuracy: 0.8253 - val_loss: 0.4048 - val_accuracy: 0.8604\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5120 - accuracy: 0.8264 - val_loss: 0.4028 - val_accuracy: 0.8606\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5104 - accuracy: 0.8256 - val_loss: 0.4008 - val_accuracy: 0.8620\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5023 - accuracy: 0.8285 - val_loss: 0.3988 - val_accuracy: 0.8631\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5065 - accuracy: 0.8270 - val_loss: 0.3970 - val_accuracy: 0.8622\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.5036 - accuracy: 0.8269 - val_loss: 0.3951 - val_accuracy: 0.8639\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4959 - accuracy: 0.8316 - val_loss: 0.3937 - val_accuracy: 0.8639\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4922 - accuracy: 0.8326 - val_loss: 0.3916 - val_accuracy: 0.8648\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4885 - accuracy: 0.8342 - val_loss: 0.3896 - val_accuracy: 0.8649\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4882 - accuracy: 0.8340 - val_loss: 0.3882 - val_accuracy: 0.8653\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4844 - accuracy: 0.8339 - val_loss: 0.3866 - val_accuracy: 0.8662\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4878 - accuracy: 0.8340 - val_loss: 0.3856 - val_accuracy: 0.8661\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4777 - accuracy: 0.8360 - val_loss: 0.3837 - val_accuracy: 0.8673\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4731 - accuracy: 0.8389 - val_loss: 0.3827 - val_accuracy: 0.8674\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4757 - accuracy: 0.8382 - val_loss: 0.3807 - val_accuracy: 0.8683\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4737 - accuracy: 0.8390 - val_loss: 0.3795 - val_accuracy: 0.8680\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4684 - accuracy: 0.8411 - val_loss: 0.3775 - val_accuracy: 0.8692\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4660 - accuracy: 0.8410 - val_loss: 0.3766 - val_accuracy: 0.8696\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4670 - accuracy: 0.8412 - val_loss: 0.3749 - val_accuracy: 0.8700\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4668 - accuracy: 0.8408 - val_loss: 0.3736 - val_accuracy: 0.8704\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4586 - accuracy: 0.8421 - val_loss: 0.3724 - val_accuracy: 0.8709\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4613 - accuracy: 0.8422 - val_loss: 0.3710 - val_accuracy: 0.8713\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4587 - accuracy: 0.8437 - val_loss: 0.3698 - val_accuracy: 0.8711\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4562 - accuracy: 0.8442 - val_loss: 0.3685 - val_accuracy: 0.8717\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4522 - accuracy: 0.8436 - val_loss: 0.3670 - val_accuracy: 0.8719\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4483 - accuracy: 0.8464 - val_loss: 0.3662 - val_accuracy: 0.8728\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4496 - accuracy: 0.8459 - val_loss: 0.3651 - val_accuracy: 0.8734\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4493 - accuracy: 0.8467 - val_loss: 0.3635 - val_accuracy: 0.8734\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4435 - accuracy: 0.8479 - val_loss: 0.3629 - val_accuracy: 0.8738\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4448 - accuracy: 0.8478 - val_loss: 0.3617 - val_accuracy: 0.8742\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4421 - accuracy: 0.8495 - val_loss: 0.3604 - val_accuracy: 0.8752\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4423 - accuracy: 0.8482 - val_loss: 0.3590 - val_accuracy: 0.8752\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4386 - accuracy: 0.8500 - val_loss: 0.3585 - val_accuracy: 0.8759\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4372 - accuracy: 0.8508 - val_loss: 0.3573 - val_accuracy: 0.8766\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4387 - accuracy: 0.8493 - val_loss: 0.3561 - val_accuracy: 0.8768\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4327 - accuracy: 0.8511 - val_loss: 0.3549 - val_accuracy: 0.8765\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4349 - accuracy: 0.8490 - val_loss: 0.3544 - val_accuracy: 0.8768\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4329 - accuracy: 0.8514 - val_loss: 0.3535 - val_accuracy: 0.8776\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4297 - accuracy: 0.8528 - val_loss: 0.3525 - val_accuracy: 0.8780\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4267 - accuracy: 0.8525 - val_loss: 0.3520 - val_accuracy: 0.8784\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4228 - accuracy: 0.8542 - val_loss: 0.3505 - val_accuracy: 0.8792\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4255 - accuracy: 0.8563 - val_loss: 0.3500 - val_accuracy: 0.8788\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4221 - accuracy: 0.8549 - val_loss: 0.3489 - val_accuracy: 0.8795\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4184 - accuracy: 0.8564 - val_loss: 0.3479 - val_accuracy: 0.8798\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4165 - accuracy: 0.8579 - val_loss: 0.3476 - val_accuracy: 0.8795\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4210 - accuracy: 0.8563 - val_loss: 0.3461 - val_accuracy: 0.8806\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4151 - accuracy: 0.8581 - val_loss: 0.3450 - val_accuracy: 0.8807\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4156 - accuracy: 0.8572 - val_loss: 0.3443 - val_accuracy: 0.8806\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4108 - accuracy: 0.8585 - val_loss: 0.3431 - val_accuracy: 0.8810\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4089 - accuracy: 0.8601 - val_loss: 0.3424 - val_accuracy: 0.8808\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4081 - accuracy: 0.8599 - val_loss: 0.3417 - val_accuracy: 0.8809\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4117 - accuracy: 0.8604 - val_loss: 0.3412 - val_accuracy: 0.8820\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4072 - accuracy: 0.8604 - val_loss: 0.3404 - val_accuracy: 0.8816\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4092 - accuracy: 0.8607 - val_loss: 0.3398 - val_accuracy: 0.8820\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4041 - accuracy: 0.8622 - val_loss: 0.3386 - val_accuracy: 0.8813\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4023 - accuracy: 0.8624 - val_loss: 0.3376 - val_accuracy: 0.8822\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4033 - accuracy: 0.8604 - val_loss: 0.3370 - val_accuracy: 0.8821\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4042 - accuracy: 0.8612 - val_loss: 0.3367 - val_accuracy: 0.8829\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.4020 - accuracy: 0.8608 - val_loss: 0.3358 - val_accuracy: 0.8836\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.3993 - accuracy: 0.8623 - val_loss: 0.3348 - val_accuracy: 0.8835\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.3994 - accuracy: 0.8646 - val_loss: 0.3335 - val_accuracy: 0.8840\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.3957 - accuracy: 0.8641 - val_loss: 0.3328 - val_accuracy: 0.8844\n",
            "Test loss: 0.3328445851802826\n",
            "Test accuracy: 0.8844000101089478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE9aiDjmHy76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2ade5702-8811-4c84-fe56-166eacab5850"
      },
      "source": [
        "#Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BDnAqerJc6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the directory to current working directory\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmACEqXEYM96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model with the name clothing_classification_model\n",
        "model.save('clothing_classification_model.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NQr-s4FUuJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ec40b103-9e0b-4bc3-b5ee-25d80e71d28f"
      },
      "source": [
        "\n",
        "# Import few more necessary libraries.\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Function to load and prepare the image in right shape\n",
        "def load_image(filename):\n",
        "\t# Load the image\n",
        "\timg = load_img(filename, grayscale=True, target_size=(28, 28))\n",
        "\t# Convert the image to array\n",
        "\timg = img_to_array(img)\n",
        "\t# Reshape the image into a sample of 1 channel\n",
        "\timg = img.reshape(1, 28, 28, 1)\n",
        "\t# Prepare it as pixel data\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\treturn img\n",
        "\n",
        "# Load an image and predict the apparel class\n",
        "img = load_image('Sandals.jpg')\n",
        "# Load the saved model\n",
        "model = load_model('clothing_classification_model.h5')\n",
        "# Predict the apparel class\n",
        "class_prediction = model.predict_classes(img)\n",
        "print(class_prediction[0])\n",
        "\n",
        "#Map apparel category with the numerical class\n",
        "if class_prediction[0] == 0:\n",
        "  product = \"T-shirt/top\"\n",
        "elif class_prediction[0] == 1:\n",
        "  product = \"Trouser\"\n",
        "elif class_prediction[0] == 2:\n",
        "  product = \"Pullover\"\n",
        "elif class_prediction[0] == 3:\n",
        "  product = \"Dress\"\n",
        "elif class_prediction[0] == 4:\n",
        "  product = \"Coat\"\n",
        "elif class_prediction[0] == 5:\n",
        "  product = \"Sandal\"\n",
        "elif class_prediction[0] == 6:\n",
        "  product = \"Shirt\"\n",
        "elif class_prediction[0] == 7:\n",
        "  product = \"Sneaker\"\n",
        "elif class_prediction[0] == 8:\n",
        "  product = \"Bag\"\n",
        "else:\n",
        "  product = \"Ankle boot\"\n",
        "\n",
        "print(product)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9255403ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "5\n",
            "Sandal\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}